# Mitigating annotation shift in cancer classification using single image generative models

```
Marta Buetas Arcas, Richard Osuala, Oliver Díaz, Karim Lekadir (2024). "Mitigating annotation shift in cancer classification using single image generative models." Departament de Matemàtiques i Informàtica, Universitat de Barcelona, Barcelona, Spain.
```
## Introduction

Artificial Intelligence (AI) has emerged as a beneficial tool to assist radiologists in breast cancer detection and diagnosis. Both the quantity and quality of available data have a direct impact on the success of these applications. One major challenge is the scarcity of labeled data, largely attributable to the extensive time, effort, and costs associated with acquiring expert annotations. This often limits the training and evaluation of AI models, causing a lack of generalisation and robustness. This issue is further exacerbated by the varying quality of available expert annotations that commonly display high intra- and inter-observer variability. This can lead to annotation shift, where a model’s performance decreases at test time if test annotations differ from their training counterparts, for example, in size, accuracy, delineation, lesion boundary and margin definition, annotation protocol or sourcing modality.
To increase classification model robustness against annotation shift, one approach is to generate additional training images that correspond to the desired annotation characteristics of the target domain (in-domain). To this end, we propose the selection of a single well-annotated in-domain training image to train a generative AI model, which, in turn, learns to synthesize an arbitrary number of variations. These variations are readily usable as additional classification model training images.

## Materials

- For this study, patches were extracted from the mammograms, including both lesion and healthy areas, in both scanned and digital formats. The technique followed for generating the patch dataset is explained in detail in the Python notebook `generate_patch_dataset.ipynb`. To generate the dataset, the [BCDR dataset](https://bcdr.eu/) needs to be downloaded. Three metadata .csv files are also generated, one for lesions, another for healthy digital patches, and a third one for scanned film patches. These files contain the corresponding data required for the study objectives, each with a unique ID for each patch. To convert the previously generated .csv files into a unified .jsonl metadata file, the Python script `csv_to_jsonl_metadata.py` is used. To simulate annotation shift, we extract patches from the lesions from more and less tightly fitting bounding boxes surrounding the lesion, i.e., with different levels of zoom. In practice, an accurate lesion delineation allows to extract a tight lesion bounding box. On the other hand, rectangle lesion annotations (e.g. performed either by human experts or by object detection models) contain varying amounts of healthy tissue surrounding the lesions. Therefore, our bounding boxes -- extracted based on different zoom levels -- simulate varying annotation protocols (annotation shift) and thus allow to measure their influence on classification performance. Thus, for each lesion, three patches are defined and extracted with different levels of zoom, capturing varying percentages of adjacent healthy tissue. Group 1 (G1) patches correspond to the most accurate bounding box defined around the original annotated lesion delineation mask. Group 2 (G2) and 3 (G3) capture patches with double (200\%) and triple (300\%) the height and width of the original bounding box, respectively. The following figure exemplifies the aforementioned lesion patches extracted from different zoom levels from a digital mammogram:
<img width="844" alt="mammogram_mask_digital (3)" src="https://github.com/MartaBuetas/EnhancingBreastCancerDiagnosis/assets/101974217/a3fa2868-3999-4383-95a4-08e1fbb5e884">

### Classifier set-up

- The first contribution in this research is the design and implementation of a high-accuracy malignancy classification model trained to distinguish cancerous from benign breast lesions. Our multiclass deep-learning classification model distinguishes between (i) healthy tissue, (ii) benign, and (iii) malignant lesions. It works at patch level, classifying regions-of-interest extracted as grayscale patches with pixel dimension of 224x224 stacked across 3 channels. Pixel values were normalised to fall within the range of 0 to 1. As classification model, a ResNet50 was used, which we initialise with weights pretrained on the ImageNet dataset, available at [PyTorch: models and pre-trained weights](https://pytorch.org/vision/stable/models.html). To optimise the training process, only the parameters of the last layer were kept trainable. For the multiclass task, there were finally 6147 trainable parameters.

- We initially assessed a binary classification task distinguishing between healthy and lesion-containing patches, yielding a test accuracy of 0.924 ± 0.009 and a test ROC-AUC (Area under the Receiver Operating Characteristic Curve) of 0.971 ± 0.009, indicating how effectively the model classifies this binary class. The `binary_pipeline.py` script contains the code required for training and testing the classifier. Subsequent experiments described in this section extended this setup to multiclass classification, classifying patches either as healthy, benign, malignant.

- Each experiment in this study ran for 100 epochs and the model from the epoch with the best validation loss was selected. Models are evaluated based on a train-validation-test split across three folds, ensuring that each patient was present in only one of the sets. The data was partitioned into 10\% for testing (344 samples), 63\% for training (2167 samples), and 27\% for validation (929 samples). The experiments were conducted with consistent hyperparameters to ensure fair comparisons between methods. These included a fixed batch size of 128, utilizing the adaptive moment optimiser (Adam) with default beta parameters ($\beta_1$=0.9 and $\beta_2$=0.999), and employing a learning rate scheduler that gradually decayed the learning rate which started at 1e-2. The scheduler had a step of 5 epochs and a gamma value of 0.1. For both the binary classification problem and the multiclass task, a binary cross-entropy loss function was employed. All experiments were run on a NVIDIA RTX 2080 Super 8GB GPU using the PyTorch library. Training the classifier for 100 epochs took approximately 3 hours in this setup. across 3 folds. 

- MULTICLASS
- 
- SINGAN

- SiFID

- 


- The remaining scripts in the main folder, with the filename extension `multiclass_pipeline.py`, have been adapted from the binary pipeline to incorporate classification for the three classes. Therefore, it is now a 3-class problem. The main modifications include changing the loss function from Binary Cross Entropy to Categorical Cross Entropy, as well as adjusting the dimensions of the last layer added to the ResNet50 model. Additionally, the option of augmenting the training data with synthetic samples generated from SinGAN models has been introduced. While the number of added samples remains the same across all cases to balance the malignant class, the number of SinGAN models used (i.e., the diversity of the generated samples) varies from 2 to 8. To maintain simplicity and clarity, separate files have been created for each number of SinGAN models used. For example, the file `2SinGANs_multiclass_pipeline.py` includes an option (specified through keyboard input) to augment the training data with synthetic samples generated from 2 SinGAN models. The same principle applies to files with 4SinGAN models, 6 SinGAN models, and 8 SinGAN models.

- The folder `SinGAN` includes all the required files for training a SinGAN model from a specific training image, generating synthetic samples, and validating them through the SiFID metric presented by [Shaham, Dekel, and Michaeli](https://ui.adsabs.harvard.edu/abs/2019arXiv190501164R/abstract). It is a clone of [the official pytorch implementation of the paper: "SinGAN: Learning a Generative Model from a Single Natural Image"](https://github.com/tamarott/SinGAN), with some parameters modified and some additional files for the convenience of the development fo this project. Firstly, the script `input_preprocess.py` was added to preprocess the patches, as it is needed as input for the SinGAN model. Secondly, the script `generating_singan.py` was added to train the desired SinGAN models using the preprocessed patches and generate new images from the trained model. The original `config.py` file was modified to have a pyramid scale factor of 0.8. Therefore, the resolution of the image is reduced by 20% when passing to the next scale. Finally, a new file named `SIFID_evaluation.ipynb` was created to compute the SiFID metric for the generated samples.

